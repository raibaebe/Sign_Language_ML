{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Gesture Recognition\n",
    "\n",
    "CNN-based recognition of 24 static ASL hand gestures (A-I, K-Y) trained on Sign Language MNIST.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Dataset loading\n",
    "2. Data preprocessing & augmentation\n",
    "3. CNN model architecture\n",
    "4. Training\n",
    "5. Evaluation (accuracy, confusion matrix, learning curve)\n",
    "6. Prediction on sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Upload `sign_mnist_train.csv` and `sign_mnist_test.csv` from [Kaggle](https://www.kaggle.com/datasets/datamunge/sign-language-mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Upload sign_mnist_train.csv and sign_mnist_test.csv\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 28\n",
    "NUM_CLASSES = 25\n",
    "LABEL_TO_LETTER = {i: chr(ord('A') + i + (1 if i >= 9 else 0)) for i in range(25)}\n",
    "LETTER_TO_LABEL = {v: k for k, v in LABEL_TO_LETTER.items()}\n",
    "\n",
    "def load_csv(filepath):\n",
    "    \"\"\"Load a Sign Language MNIST CSV. Returns (images, labels).\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    labels = df.iloc[:, 0].values.astype(np.int32)\n",
    "    images = df.iloc[:, 1:].values.astype(np.float32).reshape(-1, IMG_SIZE, IMG_SIZE)\n",
    "    return images, labels\n",
    "\n",
    "train_images, train_labels = load_csv(\"sign_mnist_train.csv\")\n",
    "test_images, test_labels = load_csv(\"sign_mnist_test.csv\")\n",
    "\n",
    "print(f\"Training samples: {len(train_images)}\")\n",
    "print(f\"Test samples:     {len(test_images)}\")\n",
    "print(f\"Image shape:      {train_images[0].shape}\")\n",
    "print(f\"Label range:      {train_labels.min()} - {train_labels.max()}\")\n",
    "print(f\"Unique classes:   {len(np.unique(train_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "letters = [LABEL_TO_LETTER[u] for u in unique]\n",
    "ax.bar(letters, counts, color=\"steelblue\")\n",
    "ax.set_xlabel(\"Letter\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Training Set Class Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images\n",
    "fig, axes = plt.subplots(3, 8, figsize=(14, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.random.randint(len(train_images))\n",
    "    ax.imshow(train_images[idx], cmap=\"gray\")\n",
    "    ax.set_title(LABEL_TO_LETTER[train_labels[idx]], fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "fig.suptitle(\"Sample Training Images\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(images, labels):\n",
    "    \"\"\"Normalize to [0,1], reshape to (N,28,28,1), one-hot encode labels.\"\"\"\n",
    "    images = images.astype(np.float32) / 255.0\n",
    "    images = images.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    labels = to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "    return images, labels\n",
    "\n",
    "X_train_full, y_train_full = preprocess_pipeline(train_images, train_labels)\n",
    "X_test, y_test = preprocess_pipeline(test_images, test_labels)\n",
    "\n",
    "# Train / validation split (90/10)\n",
    "val_split = 0.1\n",
    "num_val = int(len(X_train_full) * val_split)\n",
    "idx = np.random.permutation(len(X_train_full))\n",
    "\n",
    "X_val, y_val = X_train_full[idx[:num_val]], y_train_full[idx[:num_val]]\n",
    "X_train, y_train = X_train_full[idx[num_val:]], y_train_full[idx[num_val:]]\n",
    "\n",
    "print(f\"Train: {len(X_train)}  Val: {len(X_val)}  Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "augmenter.fit(X_train)\n",
    "\n",
    "# Visualize augmented samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "sample = X_train[0:1]\n",
    "for ax in axes.flat:\n",
    "    aug_img = augmenter.random_transform(sample[0])\n",
    "    ax.imshow(aug_img.squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "fig.suptitle(\"Augmented Samples (same source image)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, 3, activation=\"relu\", padding=\"same\", input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, 3, activation=\"relu\", padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = build_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    augmenter.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = history.history\n",
    "epochs = range(1, len(h[\"accuracy\"]) + 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(epochs, h[\"accuracy\"], \"b-o\", markersize=4, label=\"Training\")\n",
    "ax1.plot(epochs, h[\"val_accuracy\"], \"r-o\", markersize=4, label=\"Validation\")\n",
    "ax1.set_title(\"Model Accuracy\", fontsize=14)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(epochs, h[\"loss\"], \"b-o\", markersize=4, label=\"Training\")\n",
    "ax2.plot(epochs, h[\"val_loss\"], \"r-o\", markersize=4, label=\"Validation\")\n",
    "ax2.set_title(\"Model Loss\", fontsize=14)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Learning Curve\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc * 100:.2f}%)\")\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix & per-class metrics\n",
    "y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "class_names = [LABEL_TO_LETTER[i] for i in range(NUM_CLASSES)]\n",
    "\n",
    "cm = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"\\n{'':>6s} {'prec':>6s} {'rec':>6s} {'f1':>6s} {'n':>6s}\")\n",
    "print(\"-\" * 32)\n",
    "for i in range(NUM_CLASSES):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    print(f\"{class_names[i]:>6s} {prec:>6.3f} {rec:>6.3f} {f1:>6.3f} {int(cm[i].sum()):>6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names, linewidths=0.5)\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 6, figsize=(14, 7))\n",
    "indices = np.random.choice(len(X_test), 18, replace=False)\n",
    "\n",
    "for ax, idx in zip(axes.flat, indices):\n",
    "    pred = y_pred[idx]\n",
    "    true = y_true[idx]\n",
    "    ax.imshow(X_test[idx].squeeze(), cmap=\"gray\")\n",
    "    color = \"green\" if pred == true else \"red\"\n",
    "    ax.set_title(f\"P:{LABEL_TO_LETTER[pred]} T:{LABEL_TO_LETTER[true]}\", color=color, fontsize=11)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Sample Predictions (green=correct, red=wrong)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_model.h5\")\n",
    "print(\"Model saved to trained_model.h5\")\n",
    "\n",
    "# Download the model file\n",
    "files.download(\"trained_model.h5\")"
   ]
  }
 ]
}
